{
  "hash": "b961f8a25907826fab5c3a93bb7fc08b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Interactive Linear Regression Lab with Python\nauthor: Sandeep N\ndate: '2025-06-06'\nformat:\n  live-html:\n    theme:\n      light:\n        - cosmo\n        - styles.scss\n      dark:\n        - darkly\n        - styles-dark.scss\n    mainfont: Inter\n    code-copy: true\n    toc: true\n    code-tools: false\n    code-overflow: wrap\ncategories:\n  - Regression\n  - python\n  - Machine Learning\nthebe:\n  use_thebe: true\n  binderOptions:\n    repo: sandeepnarasimhan/Neural-Notes\n    ref: main\n    binderUrl: https://mybinder.org\n---\n\n[![Tweet](https://img.shields.io/badge/Tweet-Share_on_Twitter-1DA1F2?logo=twitter)](https://twitter.com/intent/tweet?text=Check%20out%20this%20post!&url=https%3A%2F%2Fsandeepnarasimhan.github.io%2FNeural-Notes%2Fposts%2FVenvs.html)\n[![LinkedIn](https://img.shields.io/badge/Share-LinkedIn-blue?logo=linkedin)](https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fsandeepnarasimhan.github.io%2FNeural-Notes%2Fposts%2FVenvs.html)\n[![Email](https://img.shields.io/badge/Email-Share_via_Email-red?logo=gmail)](mailto:?subject=Interesting%20Post&body=Check%20out%20this%20blog%20post%3A%20https%3A%2F%2Fsandeepnarasimhan.github.io%2FNeural-Notes%2Fposts%2FVenvs.html)\n[![WhatsApp](https://img.shields.io/badge/WhatsApp-Share-25D366?logo=whatsapp&logoColor=white)](https://api.whatsapp.com/send?text=Check%20out%20this%20blog%20post%3A%20https%3A%2F%2Fsandeepnarasimhan.github.io%2FNeural-Notes%2Fposts%2FVenvs.html)\n[![Reddit](https://img.shields.io/badge/Reddit-Share-orange?logo=reddit)](https://www.reddit.com/submit?url=https%3A%2F%2Fsandeepnarasimhan.github.io%2FNeural-Notes%2Fposts%2FVenvs.html&title=Virtual%20Environments)\n\n\n\nThis interactive lab demonstrates a simple linear regression using `scikit-learn`.\n\nYou can **click on the code blocks and run them interactively** in your browser!\n\n---\n\n![git](images/SLR.png){**Source: Google Gemini**, height=500px, width= 1000px }\n\n## What is Linear Regression?\n\nLinear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It aims to fit a linear equation to observed data, enabling predictions and understanding of the underlying relationships.\n\n### Why Learn Linear Regression?\n- Interpretability: Coefficients provide insights into the influence of each predictor.\n- Foundation: Serves as a stepping stone to more complex models.\n- Applicability: Widely used in various fields like economics, biology, and engineering.\n\nThe simplest form (simple linear regression) fits a line:\n\ny=Œ≤0+Œ≤1x+Œµ\n\nwhere:  \n\n- y: Dependent variable\n- x: Independent variable\n- Œ≤0 is the intercept\n- Œ≤1 is the slope coefficient\n- Œµ is the error term\n\nObjective:\n\nEstimate ùõΩ0 and ùõΩ1 such that the sum of squared residuals (differences between observed and predicted values) is minimized.\n\n---\n\n### Where is Linear Regression Applied?\n- Predicting housing prices based on features like size and location.\n- Estimating sales based on advertising spend.\n- Assessing the impact of education level on income.\n\n## Implementing Linear Regression in Python\n\n### Import libraries and generate a synthetic data\n```{pyodide}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate synthetic data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3x + noise\n\nplt.scatter(X, y, alpha=0.6)\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Sample Data')\nplt.show()\n```\n\n### Fit Linear Regression Model\n```{pyodide}\nmodel = LinearRegression() # Create a Linear Regression model object\nmodel.fit(X, y)            # Fit the model (i.e., learn the best-fit line)\n\nprint(f\"Intercept (Œ≤‚ÇÄ): {model.intercept_[0]:.2f}\")\nprint(f\"Coefficient (Œ≤‚ÇÅ): {model.coef_[0][0]:.2f}\")\n```\n\n**model = LinearRegression()**\nThis creates an instance of the LinearRegression class from sklearn.linear_model.  \nAt this point, the model exists but has not yet seen any data ‚Äî it's an empty shell.  \n  \nmodel.fit(X, y)\nThis fits the model to the data ‚Äî it learns the relationship between the input (X) and output (y) by:\n\n- Calculating the best-fitting straight line that minimizes the error (specifically, the sum of squared differences between actual and predicted values).\n\n- Internally solving for the coefficients ùõΩ0 (intercept) and ùõΩ1 (slope) using the Ordinary Least Squares (OLS) method.\n\n### Visualize the Regression Line\n```{pyodide}\nplt.scatter(X, y, alpha=0.6, label='Data')\nplt.plot(X, model.predict(X), color='red', label='Regression Line')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Linear Regression Fit')\nplt.legend()\nplt.show()\n```\n\nThe linear regression plot provides a visual interpretation of how well the model fits the data. The scattered blue dots represent the actual data points‚Äîeach showing a real observation of the relationship between the independent variable (X-axis) and the dependent variable (Y-axis). The red line represents the model‚Äôs predictions, also known as the line of best fit, which summarizes the linear trend the model has learned from the data. If the red line closely follows the pattern of the dots, it indicates a strong linear relationship and a good model fit. Ideally, the data points should be distributed evenly around the red line with minimal vertical distance (residuals), suggesting that the model is accurately capturing the underlying trend.\n\n### Model validation\n```{pyodide}\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, root_mean_squared_error\n\ny_pred = model.predict(X)\nr2 = r2_score(y, y_pred)\nmae = mean_absolute_error(y, y_pred)\nmse = mean_squared_error(y, y_pred)\nrmse = root_mean_squared_error(y, y_pred)\n\n\nprint(f\"R-squared: {r2:.4f}\")\nprint(f\"Mean Absolute Error (MAE): {mae:.4f}\")\nprint(f\"Mean Squared Error (MSE): {mse:.4f}\")\nprint(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n```\n\n\n1. R¬≤ measures the proportion of the variance in the target variable that is explained by the features. It tells you how well your model captures the variability in the data. (Range: 0-1) \n2. MAE gives the average magnitude of the errors in a set of predictions, without considering direction (i.e., all errors are positive).\n3. MSE calculates the average of the squared errors. Squaring gives more weight to large errors, making it sensitive to outliers.\n4. RMSE is the square root of MSE, bringing the error back to the same units as the target variable ‚Äî making it more interpretable.\n\n\n| Metric | Meaning                    | Good Value     |\n|--------|----------------------------|----------------|\n| R¬≤     | % of variance explained    | Closer to 1    |\n| MAE    | Avg. absolute error        | Closer to 0    |\n| MSE    | Avg. squared error         | Closer to 0    |\n| RMSE   | Square root of MSE         | Closer to 0    |\n\n\n## Make Predictions with Your Own Input\n\nOnce the model has learned the relationship between X and y, you can use it to:\n- Predict outcomes for new inputs\n- Test the model on new data (validation/test set)\n- Forecast future values (e.g., demand, prices)\n\n### Change the value of x_new below and rerun to predict a new output.\n```{pyodide}\n# You can edit this value to predict for different inputs!\nx_new = 1.5  # <-- Try changing this!\n\nx_new_array = np.array([[x_new]])\ny_pred = model.predict(x_new_array)\n\nprint(f\"Predicted y for x={x_new}: {y_pred[0][0]:.2f}\")\n```\n\n\n## Conclusion\nLinear regression is one of the most fundamental and widely used techniques for modeling the relationship between a dependent variable and one or more independent variables. By fitting a straight line through the data, it provides a clear and interpretable way to understand how changes in inputs affect the output. In this post, we explored how to implement linear regression in Python, interpret the fitted model visually, and evaluate its performance using key metrics like R-squared, MAE, MSE, and RMSE.  \n\nwhile linear regression is simple and powerful, its effectiveness depends on how well the underlying assumptions hold true, such as linearity, homoscedasticity, independence, and normality of residuals. Validating these assumptions is crucial for drawing reliable conclusions from your model.\n\nFeel free to experiment further by changing inputs, exploring multiple features, or diving deeper into residual analysis to strengthen your grasp on this important topic.\n\n\n## Connect with Me\n\n- [![GitHub](https://img.shields.io/badge/GitHub-@SandeepNarasimhan-black?logo=github)](https://github.com/SandeepNarasimhan)  \n- [![LinkedIn](https://img.shields.io/badge/LinkedIn-Sandeep%20Narasimhan-blue?logo=linkedin)](https://www.linkedin.com/in/sandeep-narasimhan-45769295)  \n- [![Email](https://img.shields.io/badge/Email-sandeepsanpande@gmail.com-red?logo=gmail)](mailto:sandeepsanpande@gmail.com)\n\n::: {.callout-caution title=\"Desclaimer\"}\nI use AI tools to assist in writing and drafting some of the content on this blog. but all content is reviewed and edited by me for accuracy and clarity.\n:::\n\n## üí¨ Comments\n\n<div id=\"utterances-thread\"></div>\n<script src=\"https://utteranc.es/client.js\"\n        repo=\"SandeepNarasimhan/Neural-Notes\"\n        issue-term=\"pathname\"\n        label=\"üí¨ comment\"\n        theme=\"github-light\"\n        crossorigin=\"anonymous\"\n        async>\n</script>\n\n",
    "supporting": [
      "regression_lab_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}