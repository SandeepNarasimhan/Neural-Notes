---
title: "Interactive Linear Regression Lab with Python"
author: "Sandeep N"
date: 2025-06-06
format:
  live-html:
    theme:
      light: [cosmo, styles.scss]
      dark: [darkly, styles-dark.scss]
    mainfont: "Inter"
    code-copy: true
    toc: true
    code-tools: false
    code-overflow: wrap
categories: [Regression, python, Machine Learning]
jupyter: python3
thebe:
  use_thebe: true
  binderOptions:
    repo: sandeepnarasimhan/Neural-Notes
    ref: main
    binderUrl: https://mybinder.org
---
[![Tweet](https://img.shields.io/badge/Tweet-Share_on_Twitter-1DA1F2?logo=twitter)](https://twitter.com/intent/tweet?text=Check%20out%20this%20post!&url=https%3A%2F%2Fsandeepnarasimhan.github.io%2FNeural-Notes%2Fposts%2FVenvs.html)
[![LinkedIn](https://img.shields.io/badge/Share-LinkedIn-blue?logo=linkedin)](https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fsandeepnarasimhan.github.io%2FNeural-Notes%2Fposts%2FVenvs.html)
[![Email](https://img.shields.io/badge/Email-Share_via_Email-red?logo=gmail)](mailto:?subject=Interesting%20Post&body=Check%20out%20this%20blog%20post%3A%20https%3A%2F%2Fsandeepnarasimhan.github.io%2FNeural-Notes%2Fposts%2FVenvs.html)
[![WhatsApp](https://img.shields.io/badge/WhatsApp-Share-25D366?logo=whatsapp&logoColor=white)](https://api.whatsapp.com/send?text=Check%20out%20this%20blog%20post%3A%20https%3A%2F%2Fsandeepnarasimhan.github.io%2FNeural-Notes%2Fposts%2FVenvs.html)
[![Reddit](https://img.shields.io/badge/Reddit-Share-orange?logo=reddit)](https://www.reddit.com/submit?url=https%3A%2F%2Fsandeepnarasimhan.github.io%2FNeural-Notes%2Fposts%2FVenvs.html&title=Virtual%20Environments)



This interactive lab demonstrates a simple linear regression using `scikit-learn`.

You can **click on the code blocks and run them interactively** in your browser!

---

![git](images/SLR.png){**Source: Google Gemini**, height=500px, width= 1000px }

## What is Linear Regression?

Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It aims to fit a linear equation to observed data, enabling predictions and understanding of the underlying relationships.

### Why Learn Linear Regression?
- Interpretability: Coefficients provide insights into the influence of each predictor.
- Foundation: Serves as a stepping stone to more complex models.
- Applicability: Widely used in various fields like economics, biology, and engineering.

The simplest form (simple linear regression) fits a line:

y=Œ≤0+Œ≤1x+Œµ

where:  

- y: Dependent variable
- x: Independent variable
- Œ≤0 is the intercept
- Œ≤1 is the slope coefficient
- Œµ is the error term

Objective:

Estimate ùõΩ0 and ùõΩ1 such that the sum of squared residuals (differences between observed and predicted values) is minimized.

---

### Where is Linear Regression Applied?
- Predicting housing prices based on features like size and location.
- Estimating sales based on advertising spend.
- Assessing the impact of education level on income.

## Implementing Linear Regression in Python

### Import libraries and generate a synthetic data
```{pyodide}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3x + noise

plt.scatter(X, y, alpha=0.6)
plt.xlabel('X')
plt.ylabel('y')
plt.title('Sample Data')
plt.show()
```

### Fit Linear Regression Model
```{pyodide}
model = LinearRegression() # Create a Linear Regression model object
model.fit(X, y)            # Fit the model (i.e., learn the best-fit line)

print(f"Intercept (Œ≤‚ÇÄ): {model.intercept_[0]:.2f}")
print(f"Coefficient (Œ≤‚ÇÅ): {model.coef_[0][0]:.2f}")
```

**model = LinearRegression()**
This creates an instance of the LinearRegression class from sklearn.linear_model.  
At this point, the model exists but has not yet seen any data ‚Äî it's an empty shell.  
  
model.fit(X, y)
This fits the model to the data ‚Äî it learns the relationship between the input (X) and output (y) by:

- Calculating the best-fitting straight line that minimizes the error (specifically, the sum of squared differences between actual and predicted values).

- Internally solving for the coefficients ùõΩ0 (intercept) and ùõΩ1 (slope) using the Ordinary Least Squares (OLS) method.

### Visualize the Regression Line
```{pyodide}
plt.scatter(X, y, alpha=0.6, label='Data')
plt.plot(X, model.predict(X), color='red', label='Regression Line')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression Fit')
plt.legend()
plt.show()
```

The linear regression plot provides a visual interpretation of how well the model fits the data. The scattered blue dots represent the actual data points‚Äîeach showing a real observation of the relationship between the independent variable (X-axis) and the dependent variable (Y-axis). The red line represents the model‚Äôs predictions, also known as the line of best fit, which summarizes the linear trend the model has learned from the data. If the red line closely follows the pattern of the dots, it indicates a strong linear relationship and a good model fit. Ideally, the data points should be distributed evenly around the red line with minimal vertical distance (residuals), suggesting that the model is accurately capturing the underlying trend.

### Model validation
```{pyodide}
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, root_mean_squared_error

y_pred = model.predict(X)
r2 = r2_score(y, y_pred)
mae = mean_absolute_error(y, y_pred)
mse = mean_squared_error(y, y_pred)
rmse = root_mean_squared_error(y, y_pred)


print(f"R-squared: {r2:.4f}")
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
```


1. R¬≤ measures the proportion of the variance in the target variable that is explained by the features. It tells you how well your model captures the variability in the data. (Range: 0-1) 
2. MAE gives the average magnitude of the errors in a set of predictions, without considering direction (i.e., all errors are positive).
3. MSE calculates the average of the squared errors. Squaring gives more weight to large errors, making it sensitive to outliers.
4. RMSE is the square root of MSE, bringing the error back to the same units as the target variable ‚Äî making it more interpretable.


| Metric | Meaning                    | Good Value     |
|--------|----------------------------|----------------|
| R¬≤     | % of variance explained    | Closer to 1    |
| MAE    | Avg. absolute error        | Closer to 0    |
| MSE    | Avg. squared error         | Closer to 0    |
| RMSE   | Square root of MSE         | Closer to 0    |


## Make Predictions with Your Own Input

Once the model has learned the relationship between X and y, you can use it to:
- Predict outcomes for new inputs
- Test the model on new data (validation/test set)
- Forecast future values (e.g., demand, prices)

### Change the value of x_new below and rerun to predict a new output.
```{pyodide}
# You can edit this value to predict for different inputs!
x_new = 1.5  # <-- Try changing this!

x_new_array = np.array([[x_new]])
y_pred = model.predict(x_new_array)

print(f"Predicted y for x={x_new}: {y_pred[0][0]:.2f}")
```


## Conclusion
Linear regression is one of the most fundamental and widely used techniques for modeling the relationship between a dependent variable and one or more independent variables. By fitting a straight line through the data, it provides a clear and interpretable way to understand how changes in inputs affect the output. In this post, we explored how to implement linear regression in Python, interpret the fitted model visually, and evaluate its performance using key metrics like R-squared, MAE, MSE, and RMSE.  

while linear regression is simple and powerful, its effectiveness depends on how well the underlying assumptions hold true, such as linearity, homoscedasticity, independence, and normality of residuals. Validating these assumptions is crucial for drawing reliable conclusions from your model.

Feel free to experiment further by changing inputs, exploring multiple features, or diving deeper into residual analysis to strengthen your grasp on this important topic.


## Connect with Me

- [![GitHub](https://img.shields.io/badge/GitHub-@SandeepNarasimhan-black?logo=github)](https://github.com/SandeepNarasimhan)  
- [![LinkedIn](https://img.shields.io/badge/LinkedIn-Sandeep%20Narasimhan-blue?logo=linkedin)](https://www.linkedin.com/in/sandeep-narasimhan-45769295)  
- [![Email](https://img.shields.io/badge/Email-sandeepsanpande@gmail.com-red?logo=gmail)](mailto:sandeepsanpande@gmail.com)

::: {.callout-caution title="Desclaimer"}
I use AI tools to assist in writing and drafting some of the content on this blog. but all content is reviewed and edited by me for accuracy and clarity.
:::

## üí¨ Comments

<div id="utterances-thread"></div>
<script src="https://utteranc.es/client.js"
        repo="SandeepNarasimhan/Neural-Notes"
        issue-term="pathname"
        label="üí¨ comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>