[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Neural Notes",
    "section": "",
    "text": "Welcome to my blog!\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nDescriptive Statistics\n\n\n\n\n\n\nJul 18, 2025\n\n\n\n\n\n\n\n\n\n\n\nStreamline Your ML Workflow with Scikit-learn Pipelines\n\n\n\n\n\n\nJun 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nInteractive Linear Regression Lab with Python\n\n\n\n\n\n\nJun 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nVersion Control Made Easy: Introduction to Git and GitHub\n\n\n\n\n\n\nMay 30, 2025\n\n\n\n\n\n\n\n\n\n\n\nVirtual Environments in ML Projects: Are They Worth the Effort?\n\n\n\n\n\n\nMay 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nA Beginner‚Äôs Guide to Data Science and Machine Learning\n\n\n\n\n\n\nMay 28, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nDescriptive Statistics\n\n\n\n\n\n\nJul 18, 2025\n\n\n\n\n\n\n\n\n\n\n\nStreamline Your ML Workflow with Scikit-learn Pipelines\n\n\n\n\n\n\nJun 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nInteractive Linear Regression Lab with Python\n\n\n\n\n\n\nJun 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nVersion Control Made Easy: Introduction to Git and GitHub\n\n\n\n\n\n\nMay 30, 2025\n\n\n\n\n\n\n\n\n\n\n\nVirtual Environments in ML Projects: Are They Worth the Effort?\n\n\n\n\n\n\nMay 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nA Beginner‚Äôs Guide to Data Science and Machine Learning\n\n\n\n\n\n\nMay 28, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Venvs.html#introduction",
    "href": "posts/Venvs.html#introduction",
    "title": "Virtual Environments in ML Projects: Are They Worth the Effort?",
    "section": "Introduction",
    "text": "Introduction\nMany beginners (including me ü§òüèΩü§¶üèΩ‚Äç‚ôÇÔ∏è) in data science and machine learning often rush into building models without laying the foundation for a clean, maintainable, and scalable workflow. In this post, I‚Äôll walk you through why setting up a virtual environment and organizing your ML project files isn‚Äôt just helpful it‚Äôs essential.\nWhether you‚Äôre a beginner or not, these practices will keep your projects healthy as they grow.\n{Source: ChatGPT, height=500px, width= 1000px}"
  },
  {
    "objectID": "posts/Venvs.html#learning-the-hard-way",
    "href": "posts/Venvs.html#learning-the-hard-way",
    "title": "Virtual Environments in ML Projects: Are They Worth the Effort?",
    "section": "Learning the Hard Way",
    "text": "Learning the Hard Way\nI didn‚Äôt come from a traditional programming background. I didn‚Äôt learn about virtual environments or project organization in a classroom or bootcamp. For the longest time, I was just running pip install in the global Python environment and saving my files wherever they felt convenient.\nAnd guess what? Everything seemed fine‚Ä¶ until it wasn‚Äôt.\nWhen I was working on a real-world machine learning project for a client, I had to:\n\nUse specific versions of packages (example: pandas and scikit-learn)\nDebug code that worked on my machine but failed on the client‚Äôs server.\nShare my work with another data scientist who couldn‚Äôt run any of my code.\n\nIt was chaotic.\nI was neck-deep in bugs, broken dependencies, and messy file structures. And there was no turning back, I had to fix it all in a short time.\nThat‚Äôs when I discovered virtual environments and structured project setups, not from a course, but from necessity. It wasn‚Äôt theory. It was survival.\n\n\n\n\n\n\nImportant\n\n\n\nSet up a virtual environment and organize your folders even if you‚Äôre just experimenting. it doesn‚Äôt seem urgent, until it is."
  },
  {
    "objectID": "posts/Venvs.html#why-this-matters",
    "href": "posts/Venvs.html#why-this-matters",
    "title": "Virtual Environments in ML Projects: Are They Worth the Effort?",
    "section": "Why This Matters",
    "text": "Why This Matters\n1. Dependency Isolation\nHave you ever installed a library for one project, only to find that it breaks another one? That‚Äôs dependency hell. Virtual environments let you isolate dependencies for each project, so you can use different versions of the same library in different projects without conflicts.\n2. Reproducibility\nImagine trying to run a project six months later and nothing works. Why? Because the environment changed. Virtual environments + a requirements.txt file = perfect reproducibility.\n3. Cleaner Code and Collaboration\nStructured code is easier to debug, scale, or share with teammates."
  },
  {
    "objectID": "posts/Venvs.html#setting-up-a-project-the-right-way",
    "href": "posts/Venvs.html#setting-up-a-project-the-right-way",
    "title": "Virtual Environments in ML Projects: Are They Worth the Effort?",
    "section": "Setting Up a Project the Right Way",
    "text": "Setting Up a Project the Right Way\nWhen starting a new project, it‚Äôs good practice to keep all your files organized in their own folder. Here‚Äôs how you can create a folder and move into it using your computer‚Äôs command line interface.\n\n1: Open your terminal or command prompt\n\nOn Windows, search for Command Prompt or PowerShell and open it.\nOn macOS, open the Terminal app.\nOn Linux, open your terminal application.\n\n\n\n2. Create a project folder\nmkdir ml_project\ncd ml_project\n\nmkdir ml_project Creates a new directory (folder) named ml_project in your current location. mkdir stands for make directory.\ncd ml_project Changes your current working directory to the newly created ml_project folder. cd stands for change directory.\n\n\n\n3. Create and Activate a Virtual Environment\npython3.12 -m venv venv\n\n\n\n\n\n\nNote\n\n\n\nRunning python3.12 -m venv venv does not install Python 3.12, it simply uses that version (if already installed) to create the virtual environment. You‚Äôll need to install Python 3.12 manually on your system before using this command.\n\n\nLet‚Äôs break down what each part means:\npython3.12: This specifies the Python interpreter you want to use for creating the virtual environment. In this case, it‚Äôs explicitly telling your system to use Python version 3.12. If you just used python or python3, it would use your default Python installation.\n-m: This flag stands for ‚Äúmodule.‚Äù It tells the Python interpreter to run a specified module as a script.\nvenv: This is the name of the module that Python provides for creating virtual environments.\nvenv (the second one): This is the name you are giving to your new virtual environment directory. a new directory named venv will be created in your current working directory. This directory will contain a copy of the Python interpreter, pip, and other necessary files for your isolated environment.\nthe command python3.12 -m venv venv does the following:\n\nIt uses the Python 3.12 interpreter.\nIt runs the venv module.\nIt creates a new directory named venv (the second venv in the command) in the current location. - This directory will house your isolated Python environment.\n\nAfter creating a Venv, you would typically activate the virtual environment\nsource venv/bin/activate  # macOS/Linux\n#venv\\Scripts\\activate     # Windows\n\n\n\n\n\n\nNote\n\n\n\nTo verify if a Python virtual environment (venv) is activated, check if your terminal prompt is prefixed with the environment‚Äôs name (e.g., (venv)). make sure that it is activated before installing any packages.\n\n\n\n\n4: Install Essential Packages\n#Single package at a time\npip install numpy\n\n# Multiple packages at once\npip install pandas matplotlib scikit-learn\npip: This stands for ‚ÄúPip Installs Packages.‚Äù It is the standard package installer for Python.\n\npip manages Python packages that aren‚Äôt part of the standard library.\nYou should use pip whenever you need external Python packages for your projects.\nYou can install and uninstall packages with pip.\nYou use requirements files to manage projects‚Äô dependencies.\n\n\n\n\n\n\n\nCaution\n\n\n\nTo install multiple Python packages like numpy, pandas, matplotlib, and scikit-learn, you simply list them separated by spaces, no commas needed. pip install pandas matplotlib scikit-learn\n\n\n\n\n5: Freeze Your Environment\nOnce you‚Äôve installed all the necessary libraries and your application is working as expected, freezing the environment creates a snapshot of those dependencies. This is like a ‚Äúcheckpoint‚Äù for your project‚Äôs environment.\npip freeze &gt; requirements.txt\nEvery time you add a new package with pip install or remove one with pip uninstall, it‚Äôs a good practice to update your requirements.txt file to reflect the current state of your virtual environment.\n\n\n\n\n\n\nNote\n\n\n\nFor robust project setups, use Poetry instead of pip as it offers integrated dependency management and automatic lock files (poetry.lock). Poetry\n\n\n\n\n6. To replicate the environment later by you or others\npip install -r requirements.txt\nThis command automates the process of setting up a project‚Äôs Python dependencies. Instead of manually running pip install for each package, you can just run this one command, and pip will install everything your project needs, often with the precise versions required for compatibility. This is crucial for reproducible development environments."
  },
  {
    "objectID": "posts/Venvs.html#why-project-structure-is-important",
    "href": "posts/Venvs.html#why-project-structure-is-important",
    "title": "Virtual Environments in ML Projects: Are They Worth the Effort?",
    "section": "Why Project Structure is Important",
    "text": "Why Project Structure is Important\nA clear and consistent project structure makes your code easier to understand, reuse, and maintain not just for others, but for yourself in the future. It helps separate different parts of your work like data, scripts, and notebooks, making your project more organized and scalable.\nYou don‚Äôt have to over-engineer, but a minimal structure with folders like src/, data/, and notebooks/ is a low effort way to stay organized.\n\n\n\n\n\n\nTip\n\n\n\nmkdir -p ml_project/{data,notebooks,src,models} #creates multiple directories at once\n\n\nNow, your ml_project should look something like this\nml_project/\n‚îú‚îÄ‚îÄ data/            # Raw or processed datasets\n‚îú‚îÄ‚îÄ notebooks/       # Jupyter notebooks for EDA\n‚îú‚îÄ‚îÄ src/             # Source code / scripts / modules\n‚îú‚îÄ‚îÄ models/          # Saved model files\n‚îú‚îÄ‚îÄ venv/            # Virtual environment\n‚îú‚îÄ‚îÄ requirements.txt # Package list\nSetting up a clear and organized project structure might feel like an extra step at the start, especially for small or experimental projects. However, it lays a strong foundation that helps you stay organized, makes your work easier to maintain, and prepares you for scaling up or collaborating with others."
  },
  {
    "objectID": "posts/Venvs.html#conclusion",
    "href": "posts/Venvs.html#conclusion",
    "title": "Virtual Environments in ML Projects: Are They Worth the Effort?",
    "section": "Conclusion",
    "text": "Conclusion\nUsing a venv isn‚Äôt just a good practice it‚Äôs essential. It keeps your project dependencies isolated, prevents ‚Äúdependency hell‚Äù, and ensures your code behaves consistently across machines. I‚Äôve learned this the hard way as someone who didn‚Äôt come from a programming background, it wasn‚Äôt until I hit painful roadblocks in a real-world client project that I realized the value of proper environment management.\nWhile model accuracy often takes the spotlight, it‚Äôs only one part of the bigger picture. Reproducibility, collaboration, and maintainability are just as important especially when your work moves from personal projects to production or team settings."
  },
  {
    "objectID": "posts/Venvs.html#whats-next-getting-started-with-git-github",
    "href": "posts/Venvs.html#whats-next-getting-started-with-git-github",
    "title": "Virtual Environments in ML Projects: Are They Worth the Effort?",
    "section": "What‚Äôs Next: Getting Started with Git & GitHub",
    "text": "What‚Äôs Next: Getting Started with Git & GitHub\nNow that we‚Äôve seen how important it is to structure your ML projects and isolate your environments, the next natural step is version control ‚Äî and that‚Äôs where Git and GitHub come in.\nIn my next blog post, I‚Äôll talk about:\n\nWhat Git actually is and why it‚Äôs essential\nSetting up a GitHub repository for your project"
  },
  {
    "objectID": "posts/Venvs.html#resources",
    "href": "posts/Venvs.html#resources",
    "title": "Virtual Environments in ML Projects: Are They Worth the Effort?",
    "section": "Resources",
    "text": "Resources\n\nPython Packaging User Guide ‚Äî virtualenv tutorial\nReal Python - Python Virtual Environments\n\n\nConnect with Me\n\n\n\n\n\n\n\n\n\n\n\n\n\nDesclaimer\n\n\n\nI use AI tools to assist in writing and drafting some of the content on this blog. but all content is reviewed and edited by me for accuracy and clarity."
  },
  {
    "objectID": "posts/Venvs.html#comments",
    "href": "posts/Venvs.html#comments",
    "title": "Virtual Environments in ML Projects: Are They Worth the Effort?",
    "section": "üí¨ Comments",
    "text": "üí¨ Comments"
  },
  {
    "objectID": "posts/regression_lab.html",
    "href": "posts/regression_lab.html",
    "title": "Interactive Linear Regression Lab with Python",
    "section": "",
    "text": "This interactive lab demonstrates a simple linear regression using scikit-learn.\nYou can click on the code blocks and run them interactively in your browser!\n{Source: Google Gemini, height=500px, width= 1000px }"
  },
  {
    "objectID": "posts/regression_lab.html#what-is-linear-regression",
    "href": "posts/regression_lab.html#what-is-linear-regression",
    "title": "Interactive Linear Regression Lab with Python",
    "section": "What is Linear Regression?",
    "text": "What is Linear Regression?\nLinear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It aims to fit a linear equation to observed data, enabling predictions and understanding of the underlying relationships.\n\nWhy Learn Linear Regression?\n\nInterpretability: Coefficients provide insights into the influence of each predictor.\nFoundation: Serves as a stepping stone to more complex models.\nApplicability: Widely used in various fields like economics, biology, and engineering.\n\nThe simplest form (simple linear regression) fits a line:\ny=Œ≤0+Œ≤1x+Œµ\nwhere:\n\ny: Dependent variable\nx: Independent variable\nŒ≤0 is the intercept\nŒ≤1 is the slope coefficient\nŒµ is the error term\n\nObjective:\nEstimate ùõΩ0 and ùõΩ1 such that the sum of squared residuals (differences between observed and predicted values) is minimized.\n\n\n\nWhere is Linear Regression Applied?\n\nPredicting housing prices based on features like size and location.\nEstimating sales based on advertising spend.\nAssessing the impact of education level on income."
  },
  {
    "objectID": "posts/regression_lab.html#implementing-linear-regression-in-python",
    "href": "posts/regression_lab.html#implementing-linear-regression-in-python",
    "title": "Interactive Linear Regression Lab with Python",
    "section": "Implementing Linear Regression in Python",
    "text": "Implementing Linear Regression in Python\n\nImport libraries and generate a synthetic data\n\n\n\n\n\n\n\n\nFit Linear Regression Model\n\n\n\n\n\n\nmodel = LinearRegression() This creates an instance of the LinearRegression class from sklearn.linear_model.\nAt this point, the model exists but has not yet seen any data it‚Äôs an empty shell.\nmodel.fit(X, y) This fits the model to the data it learns the relationship between the input (X) and output (y) by:\n\nCalculating the best fitting straight line that minimizes the error (specifically, the sum of squared differences between actual and predicted values).\nInternally solving for the coefficients ùõΩ0 (intercept) and ùõΩ1 (slope) using the Ordinary Least Squares (OLS) method.\n\n\n\nVisualize the Regression Line\n\n\n\n\n\n\nThe linear regression plot provides a visual interpretation of how well the model fits the data. The scattered blue dots represent the actual data points each showing a real observation of the relationship between the independent variable (X axis) and the dependent variable (Y axis). The red line represents the model‚Äôs predictions, also known as the line of best fit, which summarizes the linear trend the model has learned from the data. If the red line closely follows the pattern of the dots, it indicates a strong linear relationship and a good model fit. Ideally, the data points should be distributed evenly around the red line with minimal vertical distance (residuals), suggesting that the model is accurately capturing the underlying trend.\n\n\nModel validation\n\n\n\n\n\n\n\nR¬≤ measures the proportion of the variance in the target variable that is explained by the features. It tells you how well your model captures the variability in the data. (Range: 0-1)\nMAE gives the average magnitude of the errors in a set of predictions, without considering direction (i.e., all errors are positive).\nMSE calculates the average of the squared errors. Squaring gives more weight to large errors, making it sensitive to outliers.\nRMSE is the square root of MSE, bringing the error back to the same units as the target variable making it more interpretable.\n\n\n\n\nMetric\nMeaning\nGood Value\n\n\n\n\nR¬≤\n% of variance explained\nCloser to 1\n\n\nMAE\nAvg. absolute error\nCloser to 0\n\n\nMSE\nAvg. squared error\nCloser to 0\n\n\nRMSE\nSquare root of MSE\nCloser to 0"
  },
  {
    "objectID": "posts/regression_lab.html#make-predictions-with-your-own-input",
    "href": "posts/regression_lab.html#make-predictions-with-your-own-input",
    "title": "Interactive Linear Regression Lab with Python",
    "section": "Make Predictions with Your Own Input",
    "text": "Make Predictions with Your Own Input\nOnce the model has learned the relationship between X and y, you can use it to: - Predict outcomes for new inputs - Test the model on new data (validation/test set) - Forecast future values (e.g., demand, prices)\n\nChange the value of x_new below and rerun to predict a new output."
  },
  {
    "objectID": "posts/regression_lab.html#conclusion",
    "href": "posts/regression_lab.html#conclusion",
    "title": "Interactive Linear Regression Lab with Python",
    "section": "Conclusion",
    "text": "Conclusion\nLinear regression is one of the most fundamental and widely used techniques for modeling the relationship between a dependent variable and one or more independent variables. By fitting a straight line through the data, it provides a clear and interpretable way to understand how changes in inputs affect the output. In this post, we explored how to implement linear regression in Python, interpret the fitted model visually, and evaluate its performance using key metrics like R squared, MAE, MSE, and RMSE.\nwhile linear regression is simple and powerful, its effectiveness depends on how well the underlying assumptions hold true, such as linearity, homoscedasticity, independence, and normality of residuals. Validating these assumptions is crucial for drawing reliable conclusions from your model.\nFeel free to experiment further by changing inputs, exploring multiple features, or diving deeper into residual analysis to strengthen your grasp on this important topic."
  },
  {
    "objectID": "posts/regression_lab.html#connect-with-me",
    "href": "posts/regression_lab.html#connect-with-me",
    "title": "Interactive Linear Regression Lab with Python",
    "section": "Connect with Me",
    "text": "Connect with Me\n\n\n\n\n\n\n\n\n\n\n\n\n\nDesclaimer\n\n\n\nI use AI tools to assist in writing and drafting some of the content on this blog. but all content is reviewed and edited by me for accuracy and clarity."
  },
  {
    "objectID": "posts/regression_lab.html#comments",
    "href": "posts/regression_lab.html#comments",
    "title": "Interactive Linear Regression Lab with Python",
    "section": "üí¨ Comments",
    "text": "üí¨ Comments"
  },
  {
    "objectID": "posts/Intro to Data Science.html",
    "href": "posts/Intro to Data Science.html",
    "title": "A Beginner‚Äôs Guide to Data Science and Machine Learning",
    "section": "",
    "text": "Welcome to my first technical blog post on Neural Notes! If you‚Äôre curious about what data science is and how machine learning fits into it, you‚Äôre in the right place. Let‚Äôs break it down from the ground up with relatable examples and practical insights.\nIn today‚Äôs digital age, data is being generated at an unprecedented rate from social media interactions and online shopping to healthcare records and smart devices. Data science plays a crucial role in turning this vast sea of raw data into meaningful insights that help businesses, governments, and individuals make smarter decisions.\n{Source: ChatGPT, height=500px, width= 1000px }"
  },
  {
    "objectID": "posts/Intro to Data Science.html#why-choose-data-science-or-machine-learning-as-a-career",
    "href": "posts/Intro to Data Science.html#why-choose-data-science-or-machine-learning-as-a-career",
    "title": "A Beginner‚Äôs Guide to Data Science and Machine Learning",
    "section": "Why Choose Data Science or Machine Learning as a Career?",
    "text": "Why Choose Data Science or Machine Learning as a Career?\n\nHigh Demand, Low Supply\nLucrative Salaries\nIntellectually Stimulating\nDiverse Backgrounds Welcome"
  },
  {
    "objectID": "posts/Intro to Data Science.html#who-should-consider-this-path",
    "href": "posts/Intro to Data Science.html#who-should-consider-this-path",
    "title": "A Beginner‚Äôs Guide to Data Science and Machine Learning",
    "section": "Who Should Consider This Path?",
    "text": "Who Should Consider This Path?\n\nCurious Minds: You love asking ‚Äúwhy‚Äù and digging into data.\nProblem Solvers: Enjoy finding patterns, optimizing systems, or solving puzzles.\nCreative Thinkers: ML isn‚Äôt only math it‚Äôs about using data in innovative ways.\nCareer Changers: You‚Äôre in another domain but want a dynamic, future-facing role."
  },
  {
    "objectID": "posts/Intro to Data Science.html#what-is-data-science",
    "href": "posts/Intro to Data Science.html#what-is-data-science",
    "title": "A Beginner‚Äôs Guide to Data Science and Machine Learning",
    "section": "What is Data Science?",
    "text": "What is Data Science?\nData science is the art and science of extracting insights from data using statistics, programming, and domain knowledge. It lies at the intersection of three main disciplines:\n\nMathematics & Statistics: to make sense of data patterns\nProgramming: to clean, manipulate, and visualize data\nDomain Knowledge: to contextualize the insights\n\nThink of a data scientist as a detective finding clues (features), identifying suspects (hypotheses), and solving mysteries (business problems) using data.\nData science powers personalized recommendations on streaming platforms, fraud detection in banking, targeted marketing campaigns, and even advances in medical diagnosis. It helps us understand complex patterns, predict future trends, and optimize operations across almost every industry.\nSimply put, data science is the backbone of innovation and efficiency in our modern world. As more data continues to be created every second, the ability to analyze and interpret this data effectively is becoming one of the most valuable skills and a key driver of progress and growth in society.\nFor example: instead of writing rules to identify spam emails, you can train an ML model with thousands of examples of spam and nonspam emails it learns patterns and starts identifying spam on its own."
  },
  {
    "objectID": "posts/Intro to Data Science.html#what-is-machine-learning",
    "href": "posts/Intro to Data Science.html#what-is-machine-learning",
    "title": "A Beginner‚Äôs Guide to Data Science and Machine Learning",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\nMachine Learning (ML) is a subset of data science. It involves training algorithms to learn from data and make predictions or decisions without being explicitly programmed for every scenario.\n\n\nCode\nflowchart LR\n    A[Problem&lt;br&gt;Statement]\n    B[Data&lt;br&gt;Collection]\n    C[EDA and &lt;br&gt;Data Preprocessing]\n    D[Feature&lt;br&gt;Engineering]\n    E[Model&lt;br&gt;Training]\n    F[Model&lt;br&gt;Evaluation]\n    G{Training&lt;br&gt;Goal&lt;br&gt;Met?}\n    H[Hyperparameter&lt;br&gt;Tuning]\n    I[Model&lt;br&gt;Deployment]\n    J[Monitoring &&lt;br&gt;Maintenance]\n\n    A --&gt; B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G\n    G -- Yes --&gt; I --&gt; J --&gt; B\n    G -- No --&gt; H --&gt; E\n\n    style G fill:#2ECC71,color:#fff,font-weight:bold\n    style A fill:#D5F5E3\n    style B fill:#D5F5E3\n    style C fill:#D5F5E3\n    style D fill:#D5F5E3\n    style E fill:#D5F5E3\n    style F fill:#D5F5E3\n    style H fill:#D5F5E3\n    style I fill:#D5F5E3\n\n\n\n\n\nflowchart LR\n    A[Problem&lt;br&gt;Statement]\n    B[Data&lt;br&gt;Collection]\n    C[EDA and &lt;br&gt;Data Preprocessing]\n    D[Feature&lt;br&gt;Engineering]\n    E[Model&lt;br&gt;Training]\n    F[Model&lt;br&gt;Evaluation]\n    G{Training&lt;br&gt;Goal&lt;br&gt;Met?}\n    H[Hyperparameter&lt;br&gt;Tuning]\n    I[Model&lt;br&gt;Deployment]\n    J[Monitoring &&lt;br&gt;Maintenance]\n\n    A --&gt; B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G\n    G -- Yes --&gt; I --&gt; J --&gt; B\n    G -- No --&gt; H --&gt; E\n\n    style G fill:#2ECC71,color:#fff,font-weight:bold\n    style A fill:#D5F5E3\n    style B fill:#D5F5E3\n    style C fill:#D5F5E3\n    style D fill:#D5F5E3\n    style E fill:#D5F5E3\n    style F fill:#D5F5E3\n    style H fill:#D5F5E3\n    style I fill:#D5F5E3\n\n\n\n\n\n\nInstead of writing fixed rules, ML algorithms find patterns in data and use those patterns to make predictions, classifications, or decisions. This ability to learn and adapt from experience makes ML incredibly powerful for solving complex problems where traditional programming falls short.\n\n‚ÄúA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.‚Äù\n‚Äî Tom Mitchell, 1997\n\nMachine Learning powers many technologies we use daily voice assistants, recommendation systems, spam filters, fraud detection, and self driving cars. It allows computers to handle tasks that are too complex for traditional programming by learning directly from data.\nAs data grows and computing power increases, ML‚Äôs role in driving automation, personalization, and intelligent decision-making will only become more significant."
  },
  {
    "objectID": "posts/Intro to Data Science.html#types-of-machine-learning",
    "href": "posts/Intro to Data Science.html#types-of-machine-learning",
    "title": "A Beginner‚Äôs Guide to Data Science and Machine Learning",
    "section": "Types of Machine Learning",
    "text": "Types of Machine Learning\nMachine learning is generally categorized into three types:\n\n1. Supervised Learning\nIn supervised learning, the algorithm is trained on a labeled dataset (i.e., the input data comes with corresponding output labels).\n\n\n\n\n\n\nDid You Know?\n\n\n\nSupervised learning is the most common form of machine learning, where the model learns from labeled data.\n\n\n\nExamples:\n\nSpam Detection: Emails labeled as spam or not spam\nHouse Price Prediction: Historical house data with prices\n\n\n\nCommon Algorithms:\n\nLinear Regression\nDecision Trees\nSupport Vector Machines\nRandom Forests\nXGBoost\n\n\n\nReal-world Use Case:\nA bank wants to predict loan defaults. It uses historical customer data (age, income, credit score) labeled with whether they defaulted. The model learns this mapping and predicts for new applicants.\n\n\n\n\n2. Unsupervised Learning\nHere, the algorithm explores unlabeled data to find hidden patterns or groupings.\n\nExamples:\n\nCustomer Segmentation: Grouping users based on behavior\nAnomaly Detection: Finding unusual patterns in network traffic\n\n\n\nCommon Algorithms:\n\nKMeans Clustering\nHierarchical Clustering\nPCA (Dimensionality Reduction)\n\n\n\nReal-world Use Case:\nAn ecommerce platform uses clustering to group customers with similar purchasing behavior. It then tailors recommendations to each segment.\n\n\n\n\n3. Reinforcement Learning\nThe algorithm learns by interacting with an environment, receiving rewards or penalties for actions.\n\nExamples:\n\nGame playing AI (e.g., AlphaGo)\nRobotics (self learning movement)\nSelf driving cars (learning to steer, accelerate, brake)\n\n\n\n\n\n\n\nCaution\n\n\n\nReinforcement learning can be powerful, but it‚Äôs computationally expensive and sensitive to hyperparameters. Use it only when suitable!\n\n\n\n\nConcepts:\n\nAgent, Environment\nState, Action, Reward\nPolicy & Value functions\n\n\n\nReal-world Use Case:\nAn autonomous drone learns to navigate a complex environment by trial and error, receiving rewards for reaching waypoints safely.\n\n\n\n\n\n\n\nWarning\n\n\n\nToo much time spent on model tuning with poor data will still give poor results. Garbage in, garbage out!"
  },
  {
    "objectID": "posts/Intro to Data Science.html#whats-the-future",
    "href": "posts/Intro to Data Science.html#whats-the-future",
    "title": "A Beginner‚Äôs Guide to Data Science and Machine Learning",
    "section": "What‚Äôs the Future?",
    "text": "What‚Äôs the Future?\n\nLLMs and Generative AI are revolutionizing everything from content creation to legal assistance.\nFields like AI ethics, edge computing, healthcare AI, and green ML are just beginning."
  },
  {
    "objectID": "posts/Intro to Data Science.html#further-reading-and-resources",
    "href": "posts/Intro to Data Science.html#further-reading-and-resources",
    "title": "A Beginner‚Äôs Guide to Data Science and Machine Learning",
    "section": "Further Reading and Resources",
    "text": "Further Reading and Resources\n\nThe Elements of Statistical Learning ‚Äî Hastie, Tibshirani, Friedman\nMachine Learning Yearning ‚Äî Andrew Ng\nGoogle‚Äôs Machine Learning Crash Course\nML Course Coursera"
  },
  {
    "objectID": "posts/Intro to Data Science.html#whats-next",
    "href": "posts/Intro to Data Science.html#whats-next",
    "title": "A Beginner‚Äôs Guide to Data Science and Machine Learning",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nIn the next post, we‚Äôll dive into the Machine Learning workflow a step by step guide on how data scientists turn raw data into real world insights and predictions. From collecting data to deploying models, we‚Äôll explore the entire journey in a beginner friendly way.\nHere are some topics I‚Äôll be covering soon:\n\nThe Machine Learning Workflow Explained\nExploratory Data Analysis (EDA) for Beginners\nData Cleaning 101\nFeature Engineering\nModel Selection Simplified\nEvaluation Metrics Made Easy\n\nIn these posts, I‚Äôll also be sharing working examples and showing how to run code in Python or R\nStay tuned!"
  },
  {
    "objectID": "posts/Intro to Data Science.html#final-thoughts",
    "href": "posts/Intro to Data Science.html#final-thoughts",
    "title": "A Beginner‚Äôs Guide to Data Science and Machine Learning",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nMachine learning might sound intimidating at first, but it‚Äôs really just pattern recognition at scale. With the right questions and clean data, it can solve real world problems across finance, healthcare, retail, and beyond.\nThis blog is my way of learning out loud if you have suggestions, corrections, or want to share ideas, feel free to reach out!\n\n\nConnect with Me\n\n\n\n\n\n\n\n\n\n\n\n\n\nDesclaimer\n\n\n\nI use AI tools to assist in writing and drafting some of the content on this blog. but all content is reviewed and edited by me for accuracy and clarity."
  },
  {
    "objectID": "posts/Intro to Data Science.html#comments",
    "href": "posts/Intro to Data Science.html#comments",
    "title": "A Beginner‚Äôs Guide to Data Science and Machine Learning",
    "section": "üí¨ Comments",
    "text": "üí¨ Comments"
  },
  {
    "objectID": "posts/pipelines.html#introduction",
    "href": "posts/pipelines.html#introduction",
    "title": "Streamline Your ML Workflow with Scikit-learn Pipelines",
    "section": "Introduction",
    "text": "Introduction\nIf you‚Äôve spent any time building Machine Learning models, you know the process isn‚Äôt just about picking an algorithm and hitting ‚Äútrain.‚Äù It involves a series of sequential steps: data preprocessing, feature engineering, scaling, dimensionality reduction, and finally, model training.\n{Source: ChatGPT, height=500px, width= 1000px}\nManaging these steps independently can quickly lead to messy, repetitive code, potential data leakage, and difficulties in hyperparameter tuning. This is where Scikit-learn Pipelines come to the rescue!\nIn this post, we‚Äôll dive deep into what scikit-learn pipelines are, why they‚Äôre indispensable for any serious ML project, and how to implement them with practical code examples."
  },
  {
    "objectID": "posts/pipelines.html#the-problem-pipelines-solve-a-messy-workflow",
    "href": "posts/pipelines.html#the-problem-pipelines-solve-a-messy-workflow",
    "title": "Streamline Your ML Workflow with Scikit-learn Pipelines",
    "section": "The Problem Pipelines Solve: A Messy Workflow",
    "text": "The Problem Pipelines Solve: A Messy Workflow\nImagine you‚Äôre building a classification model. Your typical workflow might look like this:\n\nLoad Data: Read your CSV file.\nSplit Data: Separate features (X) and target (y), then split into training and testing sets.\nPreprocess Numerical Features: Apply StandardScaler to numerical columns.\nPreprocess Categorical Features: Apply OneHotEncoder to categorical columns.\nCombine Features: Put all preprocessed features back together.\nTrain Model: Fit your LogisticRegression model on the transformed training data.\nEvaluate Model: Make predictions on the transformed test data and calculate metrics.\n\nThis sequential nature means that every time you want to try a different scaling method, a new feature engineering step, or a different model, you have to manually apply those transformations to both your training and testing data. It‚Äôs tedious, error-prone, and worst of all, increases the risk of data leakage.\nData leakage occurs when information from your test set inadvertently ‚Äúleaks‚Äù into your training process. For example, if you fit a StandardScaler on your entire dataset (train + test) before splitting, your scaler learns the mean and standard deviation of the test data, giving your model an unfair advantage during evaluation. Pipelines inherently prevent this."
  },
  {
    "objectID": "posts/pipelines.html#what-are-scikit-learn-pipelines",
    "href": "posts/pipelines.html#what-are-scikit-learn-pipelines",
    "title": "Streamline Your ML Workflow with Scikit-learn Pipelines",
    "section": "What are Scikit-learn Pipelines?",
    "text": "What are Scikit-learn Pipelines?\nA scikit-learn Pipeline sequentially applies a list of transformers and a final estimator. Think of it as a single, consolidated Scikit-learn object that encapsulates your entire machine learning workflow from preprocessing to prediction.\nEach step in the pipeline is a tuple (‚Äòname‚Äô, estimator), where ‚Äòname‚Äô is a string identifier for that step (useful for hyperparameter tuning) and ‚Äòestimator‚Äô is a scikit-learn compatible transformer (like StandardScaler, SimpleImputer, OneHotEncoder) or a final estimator (like LogisticRegression, RandomForestClassifier)."
  },
  {
    "objectID": "posts/pipelines.html#practical-examples-building-pipelines-step-by-step",
    "href": "posts/pipelines.html#practical-examples-building-pipelines-step-by-step",
    "title": "Streamline Your ML Workflow with Scikit-learn Pipelines",
    "section": "Practical Examples: Building Pipelines Step-by-Step",
    "text": "Practical Examples: Building Pipelines Step-by-Step\nWe‚Äôll use the Titanic dataset from Seaborn. It contains both numerical and categorical features, making it ideal for demonstrating preprocessing steps in pipelines."
  },
  {
    "objectID": "posts/pipelines.html#load-dataset",
    "href": "posts/pipelines.html#load-dataset",
    "title": "Streamline Your ML Workflow with Scikit-learn Pipelines",
    "section": "Load Dataset",
    "text": "Load Dataset\n\nimport seaborn as sns\nimport pandas as pd\n\n# Load the Titanic dataset\ndf = sns.load_dataset(\"titanic\")\n\n# Drop rows with missing target\ndf = df.dropna(subset=[\"survived\"])\n\n# Define features and target\nX = df[[\"pclass\", \"sex\", \"age\", \"fare\", \"embarked\"]]\ny = df[\"survived\"]"
  },
  {
    "objectID": "posts/pipelines.html#numerical-features",
    "href": "posts/pipelines.html#numerical-features",
    "title": "Streamline Your ML Workflow with Scikit-learn Pipelines",
    "section": "Numerical features",
    "text": "Numerical features\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n# Numeric columns to scale\nnum_features = [\"age\", \"fare\"]\n\nnum_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler())\n])\n\nnum_pipeline\n\nPipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n                ('scaler', StandardScaler())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n                ('scaler', StandardScaler())]) SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median') StandardScaler?Documentation for StandardScalerStandardScaler() \n\n\n\nSimpleImputer(strategy=‚Äúmedian‚Äù): Fills missing numeric values with the median of the column.\nStandardScaler(): Standardizes features by removing the mean and scaling to unit variance."
  },
  {
    "objectID": "posts/pipelines.html#categorical-features",
    "href": "posts/pipelines.html#categorical-features",
    "title": "Streamline Your ML Workflow with Scikit-learn Pipelines",
    "section": "Categorical Features",
    "text": "Categorical Features\n\n# Categorical columns to encode\ncat_features = [\"pclass\", \"sex\", \"embarked\", \"age_group\"]\n\ncat_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n])\n\ncat_pipeline\n\nPipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n                ('encoder', OneHotEncoder(handle_unknown='ignore'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n                ('encoder', OneHotEncoder(handle_unknown='ignore'))]) SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent') OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') \n\n\n\nSimpleImputer(strategy=‚Äúmost_frequent‚Äù): Fills missing categorical values with the most common category.\nOneHotEncoder(): Converts categorical variables into a binary matrix.\nhandle_unknown=‚Äúignore‚Äù ensures the encoder can handle unseen categories during inference."
  },
  {
    "objectID": "posts/pipelines.html#combining-numerical-and-categorical-pipeline-into-one",
    "href": "posts/pipelines.html#combining-numerical-and-categorical-pipeline-into-one",
    "title": "Streamline Your ML Workflow with Scikit-learn Pipelines",
    "section": "Combining Numerical and Categorical pipeline into one",
    "text": "Combining Numerical and Categorical pipeline into one\n\n# Combine numeric and categorical transformers\npreprocessor = ColumnTransformer([\n    (\"num\", num_pipeline, num_features),\n    (\"cat\", cat_pipeline, cat_features)\n])\n\npreprocessor\n\nColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age', 'fare']),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['pclass', 'sex', 'embarked', 'age_group'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer?Documentation for ColumnTransformeriNot fittedColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age', 'fare']),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['pclass', 'sex', 'embarked', 'age_group'])]) num['age', 'fare'] SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median') StandardScaler?Documentation for StandardScalerStandardScaler() cat['pclass', 'sex', 'embarked', 'age_group'] SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent') OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') \n\n\n\n\n\n\n\n\nNote\n\n\n\nColumnTransformer allows different preprocessing for numerical and categorical features."
  },
  {
    "objectID": "posts/pipelines.html#adding-a-custom-transformer",
    "href": "posts/pipelines.html#adding-a-custom-transformer",
    "title": "Streamline Your ML Workflow with Scikit-learn Pipelines",
    "section": "Adding a custom Transformer",
    "text": "Adding a custom Transformer\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass AgeGroupAdder(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        X[\"age_group\"] = pd.cut(\n            X[\"age\"],\n            bins=[0, 12, 18, 50, 100],\n            labels=[\"child\", \"teen\", \"adult\", \"senior\"]\n        )\n        return X\n\n\nWe inherit from BaseEstimator and TransformerMixin to make it compatible with Scikit-learn pipelines.\npd.cut() creates a new categorical feature age_group by binning the age variable.\nThis step is reusable and testable, like any Scikit-learn transformer."
  },
  {
    "objectID": "posts/pipelines.html#full-pipeline-with-model",
    "href": "posts/pipelines.html#full-pipeline-with-model",
    "title": "Streamline Your ML Workflow with Scikit-learn Pipelines",
    "section": "Full pipeline with Model",
    "text": "Full pipeline with Model\nWe‚Äôll now build the complete pipeline by chaining the custom transformer, preprocessing, and a simple model.\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Final pipeline\nmodel_pipeline = Pipeline([\n    (\"feature_engineering\", AgeGroupAdder()),\n    (\"preprocessing\", preprocessor),\n    (\"classifier\", LogisticRegression(max_iter=1000))\n])\n\nmodel_pipeline\n\nPipeline(steps=[('feature_engineering', AgeGroupAdder()),\n                ('preprocessing',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'fare']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['pclass', 'sex', 'embarked',\n                                                   'age_group'])])),\n                ('classifier', LogisticRegression(max_iter=1000))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('feature_engineering', AgeGroupAdder()),\n                ('preprocessing',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['age', 'fare']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('encoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['pclass', 'sex', 'embarked',\n                                                   'age_group'])])),\n                ('classifier', LogisticRegression(max_iter=1000))]) AgeGroupAdderAgeGroupAdder() preprocessing: ColumnTransformer?Documentation for preprocessing: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['age', 'fare']),\n                                ('cat',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('encoder',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['pclass', 'sex', 'embarked', 'age_group'])]) num['age', 'fare'] SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median') StandardScaler?Documentation for StandardScalerStandardScaler() cat['pclass', 'sex', 'embarked', 'age_group'] SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent') OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') LogisticRegression?Documentation for LogisticRegressionLogisticRegression(max_iter=1000) \n\n\n\nPipeline: Sequentially applies steps.\nLogisticRegression: A basic classification model ideal for binary outcomes like survived."
  },
  {
    "objectID": "posts/pipelines.html#model-training-and-evaluation",
    "href": "posts/pipelines.html#model-training-and-evaluation",
    "title": "Streamline Your ML Workflow with Scikit-learn Pipelines",
    "section": "Model Training and Evaluation",
    "text": "Model Training and Evaluation\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Train the pipeline\nmodel_pipeline.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model_pipeline.predict(X_test)\n\n# Evaluate results\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\nAccuracy: 0.8044692737430168\n              precision    recall  f1-score   support\n\n           0       0.82      0.85      0.84       105\n           1       0.77      0.74      0.76        74\n\n    accuracy                           0.80       179\n   macro avg       0.80      0.80      0.80       179\nweighted avg       0.80      0.80      0.80       179\n\n\n\n\ntrain_test_split(): Splits the data into training and test sets.\nfit(): Trains the entire pipeline (including feature engineering and preprocessing).\npredict(): Makes predictions on new data.\naccuracy_score() and classification_report(): Evaluate model performance.\n\n\nScikit-learn pipelines are a powerful way to:\n\nKeep your code clean and modular\nAvoid data leakage by applying transformations only to training data\nEasily include custom steps like feature engineering\n\n\n\nConnect with Me\n\n\n\n\n\n\n\n\n\n\n\n\n\nDesclaimer\n\n\n\nI use AI tools to assist in writing and drafting some of the content on this blog. but all content is reviewed and edited by me for accuracy and clarity."
  },
  {
    "objectID": "posts/pipelines.html#comments",
    "href": "posts/pipelines.html#comments",
    "title": "Streamline Your ML Workflow with Scikit-learn Pipelines",
    "section": "üí¨ Comments",
    "text": "üí¨ Comments"
  },
  {
    "objectID": "posts/git and github.html",
    "href": "posts/git and github.html",
    "title": "Version Control Made Easy: Introduction to Git and GitHub",
    "section": "",
    "text": "You start with a file called script.py. Then it becomes script_final.py. Then script_final2.py. Eventually, you‚Äôre looking at script_final_final_really_final.py.\nIt gets messy fast.\nA system that can remember what changed, when, why and by whom ‚Äî without relying on awkward filenames or endless copies. That‚Äôs exactly where Version Control Systems (VCS) come in.\n{Source: Google Gemini, height=500px, width= 1000px }"
  },
  {
    "objectID": "posts/git and github.html#version-control-system-vcs",
    "href": "posts/git and github.html#version-control-system-vcs",
    "title": "Version Control Made Easy: Introduction to Git and GitHub",
    "section": "Version Control System (VCS)",
    "text": "Version Control System (VCS)\nhelps developers track changes in source code over time. It allows you to:\n\nRevert to previous versions of code\nCompare changes made over time\nIdentify who modified a file and when\nCollaborate with multiple contributors without overwriting each other‚Äôs work\nExperiment in branches without affecting the main codebase\n\nThink of it as a magical time machine for your code. You can move back and forth between versions, experiment freely, and maintain a clear record of all changes.\n\nThere are two major types of VCS:\n\nCentralized Version Control Systems (CVCS)\nDistributed Version Control Systems (DVCS)\n\nIn this blog, I‚Äôm going to focus on DVCS specifically, on Git.\n\n\n\n\n\n\nNote\n\n\n\nGit is the most widely used Version Control System today. It‚Äôs trusted by individual developers, open source communities, and large enterprises alike making it an essential tool in modern software development."
  },
  {
    "objectID": "posts/git and github.html#what-is-git",
    "href": "posts/git and github.html#what-is-git",
    "title": "Version Control Made Easy: Introduction to Git and GitHub",
    "section": "What is Git?",
    "text": "What is Git?\nGit is the most popular Distributed Version Control System (DVCS) used by developers across the world. It was created in 2005 by Linus Torvalds, the creator of the Linux kernel, to manage the Linux source code more efficiently.\nUnlike centralized systems, Git allows every user to have their own full copy of the repository, including its entire history."
  },
  {
    "objectID": "posts/git and github.html#git-terminology",
    "href": "posts/git and github.html#git-terminology",
    "title": "Version Control Made Easy: Introduction to Git and GitHub",
    "section": "Git Terminology",
    "text": "Git Terminology\nhigh-level overview of Git‚Äôs core concepts:\n\nRepository (repo): A directory that contains your project files and a .git folder, which tracks all changes.\nCommit: A snapshot of your files at a certain point in time. Each commit has a unique ID and message describing the change.\nBranch: A separate line of development. The default branch is usually called main or master.\nMerge: Integrates changes from one branch into another.\nClone: Copy an entire repository from a remote server to your local machine.\nPush / Pull: Push sends your local commits to a remote repository (like GitHub). Pull fetches and integrates changes from remote to local."
  },
  {
    "objectID": "posts/git and github.html#git-follows-a-three-stage-architecture",
    "href": "posts/git and github.html#git-follows-a-three-stage-architecture",
    "title": "Version Control Made Easy: Introduction to Git and GitHub",
    "section": "Git follows a three-stage architecture:",
    "text": "Git follows a three-stage architecture:\n\nWorking Tree ‚Äì Where you make changes.\nStaging Area (Index) ‚Äì Where you prepare changes for the next commit.\nRepository (.git folder) ‚Äì Where committed changes are stored permanently."
  },
  {
    "objectID": "posts/git and github.html#why-use-git",
    "href": "posts/git and github.html#why-use-git",
    "title": "Version Control Made Easy: Introduction to Git and GitHub",
    "section": "Why Use Git?",
    "text": "Why Use Git?\n\nSpeed: Most operations are local and incredibly fast.\nData Integrity: history of every change is securely stored\nBranching and Merging: Branches are lightweight and encourage experimentation.\nWidespread Adoption: Git is used in open source and enterprise environments alike. Learning Git is an essential skill for any developer or data scientist."
  },
  {
    "objectID": "posts/git and github.html#download-and-install",
    "href": "posts/git and github.html#download-and-install",
    "title": "Version Control Made Easy: Introduction to Git and GitHub",
    "section": "Download and Install",
    "text": "Download and Install\nTo start using Git, you‚Äôll need to install it on your machine. Here are the official resources:\nDownload Git\nInstallation Guide\nOnce installed, you can open a terminal (or Git Bash on Windows) and run:\ngit --version\nIf it returns a version number without any error, Git is ready to use!"
  },
  {
    "objectID": "posts/git and github.html#step-by-step-git-workflow",
    "href": "posts/git and github.html#step-by-step-git-workflow",
    "title": "Version Control Made Easy: Introduction to Git and GitHub",
    "section": "Step-by-Step Git Workflow",
    "text": "Step-by-Step Git Workflow\nWe are going to use the directory created in our last session (ml_project). please reffer Virtual environments in ML projects if you do not have it already. (You can also create a new directory if you want).\nml_project/\n‚îú‚îÄ‚îÄ data/            # Raw or processed datasets\n‚îú‚îÄ‚îÄ notebooks/       # Jupyter notebooks for EDA\n‚îú‚îÄ‚îÄ src/             # Source code / scripts / modules\n‚îú‚îÄ‚îÄ models/          # Saved model files\n‚îú‚îÄ‚îÄ venv/            # Virtual environment\n‚îú‚îÄ‚îÄ requirements.txt # Package list\n\nSet Your Name and Email (one-time setup)\ngit config --global user.name \"Your Full Name\"\ngit config --global user.email \"your.email@example.com\"\nThis configures your name and email and appears in your commit history and helps identify who made each change. Use the same email as your GitHub account to link commits correctly.\n\n\nInitialize Git in Your Project\nAdd a .gitkeep or empty .txt file inside each folder so Git can track them even if they‚Äôre empty initially:\ntouch ml_project/{data,notebooks,src,models,outputs,tests}/.gitkeep\nOnce we have created a project directory we can initialize git by\ngit init\nThis turns your folder into a Git repository by creating a hidden .git/ folder. This folder contains everything Git needs to track changes, including:\n\nCommit history\nBranches\nStaging area (index)\nConfiguration settings\n\nIt tells Git: ‚ÄúStart tracking this project.‚Äù It enables all Git features like git add, git commit, and git push.\nTo view hidden .git directory you can use\nls -l .git\n\n\n\n\n\n\nCaution\n\n\n\nOnly run git init once per project. If you delete the .git/ folder, you lose all version history.\n\n\nright after initializing git we create two files which are important\necho \"# My Project Title\" &gt; README.md\ntouch .gitignore\nREADME.md is a Markdown file that typically serves as the front page of your project.\nIt should include: - What the project does - How to install/run it - Dependencies - Usage examples and other important project details\n.gitignore tells Git which files or folders NOT to track or include in commits. Examples of what to ignore:\n\nOS/system files (.DS_Store, Thumbs.db)\nIDE/editor files (.vscode/, .idea/)\nLogs or temporary files (.log, .tmp)\nDependency folders (node_modules/, venv/)\nSensitive data (.env, credentials.json)\n\nIn our example we are going to add ‚Äúvenv/‚Äù to .gitignore\n\n\n\n\n\n\nCaution\n\n\n\nWithout a .gitignore, Git may track junk or private files that you never meant to upload.\n\n\n\n\nCheck the Current Git Status\nThis shows you: - New/untracked files - Changes that are staged or not staged - What‚Äôs ready to commit\ngit status\nYou‚Äôll see something like:\n‚Äú‚Äú‚Äù On branch master\nNo commits yet\nUntracked files: (use ‚Äúgit add ‚Ä¶‚Äù to include in what will be committed)\n.gitignore\nREADME.md\nnothing added to commit but untracked files present (use ‚Äúgit add‚Äù to track) ‚Äú‚Äú‚Äù\nNow, to track these files\n\n\nStage the File for Commit\ngit add README.md\ngit add .gitignore\nThink of git add as preparing the file to be saved (but not saving yet)\nWorking Directory ‚îÄ‚îÄ(git add)‚îÄ‚îÄ‚ñ∂ Staging Area\n\n\nCommit the File\ngit commit -m \"Add README and .giignore files\"\nThe -m flag adds a commit message describing what you did. it is always a good practice to give a proper desciption of what changes you have done in this commit. git commit takes a snapshot of your staged changes and permanently stores it in the Git repository.\n\nWhy Is git commit Important?\n\nIt records your progress in small, logical steps\nMakes it easier to undo mistakes, collaborate, and track who did what\nHelps in code reviews and project management\n\nWorking Directory ‚îÄ‚îÄ(git add)‚îÄ‚îÄ‚ñ∂ Staging Area ‚îÄ‚îÄ(git commit)‚îÄ‚îÄ‚ñ∂ Git Repository\n\n\n\nCheck the Commit History\ngit log\ngit log displays the history of commits in your repository ‚Äî from the most recent to the oldest.\nIt shows you: - Who made the commit (name and email) - When it was made - What the commit message was - A unique commit ID (SHA)"
  },
  {
    "objectID": "posts/git and github.html#wrapping-up",
    "href": "posts/git and github.html#wrapping-up",
    "title": "Version Control Made Easy: Introduction to Git and GitHub",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nIn this post, you‚Äôve taken your very first steps with Git:\n\nYou initialized a Git repository with git init\nLearned how to track changes with git add and git commit\nExplored how to inspect your project history with git log\nAnd got familiar with concepts like .gitignore and git status\nYou now have a working Git project right on your local machine.\n\nBut what if you want to back up your code, access it from anywhere, or collaborate with others?\nThat‚Äôs where GitHub comes in!"
  },
  {
    "objectID": "posts/git and github.html#whats-next",
    "href": "posts/git and github.html#whats-next",
    "title": "Version Control Made Easy: Introduction to Git and GitHub",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nIn the next blog, we‚Äôll connect your local Git project to a remote repository on GitHub. You‚Äôll learn:\n\nWhy pushing to GitHub is useful\nHow to create a GitHub repo\nHow to connect it to your local repo\nHow to push your code to the cloud!\n\nThese are the foundational steps every developer takes when beginning a new project, this is just the beginning.\n\nSource\n\nGit Documentation\nGit Book\n\nGit Cheatsheet"
  },
  {
    "objectID": "posts/git and github.html#connect-with-me",
    "href": "posts/git and github.html#connect-with-me",
    "title": "Version Control Made Easy: Introduction to Git and GitHub",
    "section": "Connect with Me",
    "text": "Connect with Me\n\n\n\n\n\n\n\n\n\n\n\n\n\nDesclaimer\n\n\n\nI use AI tools to assist in writing and drafting some of the content on this blog. but all content is reviewed and edited by me for accuracy and clarity."
  },
  {
    "objectID": "posts/git and github.html#comments",
    "href": "posts/git and github.html#comments",
    "title": "Version Control Made Easy: Introduction to Git and GitHub",
    "section": "üí¨ Comments",
    "text": "üí¨ Comments"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Welcome to Neural Notes",
    "section": "",
    "text": "{ height=500px, width= 1000px }\nHi, I‚Äôm a data scientist with a deep curiosity for both data and science. I started this blog as a personal learning space ‚Äî a place to explore new ideas, share what I‚Äôm discovering, and grow along the way.\nNeural Notes isn‚Äôt just a collection of posts ‚Äî it‚Äôs my digital lab notebook. I‚Äôm learning in public, and I‚Äôd love to learn with you.\nIf you‚Äôre reading any of my posts, feel free to share your thoughts, suggestions, or corrections. Your feedback helps me improve ‚Äî and maybe together, we can all get a little better at this data game.\nLet‚Äôs explore, experiment, and evolve ‚Äî one post at a time."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "Welcome to Neural Notes",
    "section": "About Me",
    "text": "About Me\nHi, I‚Äôm Sandeep, a data scientist from Mysore, India, with over 8 years of experience turning questions into insights and models into impact.\nI work across the stack of data science ‚Äî from exploring raw data to building machine learning models, analyzing fraud, experimenting with LLMs, and writing code in both Python and R. I enjoy solving real-world problems, especially the messy ones.\nThis blog is my personal space to learn, reflect, and grow ‚Äî not just as a professional, but as someone genuinely curious about data, systems, and how they shape our world.\nOutside of work, I like exploring music (especially metal üé∏), building side projects, and occasionally getting lost in thought over coffee and code.\nThanks for stopping by. I hope you find something here that sparks your own curiosity."
  },
  {
    "objectID": "about.html#what-youll-find-here",
    "href": "about.html#what-youll-find-here",
    "title": "Welcome to Neural Notes",
    "section": "What You‚Äôll Find Here",
    "text": "What You‚Äôll Find Here\nThis blog is where I document what I‚Äôm learning and building, including: - End-to-end machine learning projects - Data science tools and tips in Python & R - Thoughts on LLMs, model interpretability, and experimentation - Side projects, visuals, and experiments"
  },
  {
    "objectID": "about.html#tools-i-often-use",
    "href": "about.html#tools-i-often-use",
    "title": "Welcome to Neural Notes",
    "section": "Tools I Often Use",
    "text": "Tools I Often Use\n\nProgramming: Python, R, SQL\nML & AI: scikit-learn, XGBoost, LightGBM, TensorFlow, Keras, transformers\nVisualization: ggplot2, matplotlib, seaborn, Plotly\nPlatforms: Jupyter, RStudio, Streamlit, Shiny, MLflow, FastAPI\n\n\nConnect with Me"
  },
  {
    "objectID": "about.html#comments",
    "href": "about.html#comments",
    "title": "Welcome to Neural Notes",
    "section": "üí¨ Comments",
    "text": "üí¨ Comments"
  },
  {
    "objectID": "posts/Descriptive_Statistics.html",
    "href": "posts/Descriptive_Statistics.html",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "Warning\n\n\n\nüöß Work in Progress!\nHey! I‚Äôm still working on this post. If you‚Äôre interested, keep an eye out for updates here ‚Äî exciting stuff coming soon!!"
  }
]